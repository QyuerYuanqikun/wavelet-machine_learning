# wavelet-machine_learning
# Stage1 Random_Forest  
## **$step1$ 原始图片数据集收集**
$Flickr1024$是一个大规模立体图像数据集，由1024个高质量图像对组成，涵盖多种场景。尽管 $Flickr1024$ 数据集最初是为立体图像 SR 开发的，但它也用于许多其他任务，例如基于参考的 SR、立体匹配和立体图像去噪。
- 相关文件存于Image\Flickr1024 Dataset
## **$step2$ 图片加噪** 
### **(1)相关噪声原理**
- Gauss噪声
  也称正态噪声，是一种常见的随机噪声，其概率密度函数遵循正态分布（也称为高斯分布）。高斯噪声在图像处理中经常被用于模拟现实世界中的随机噪声，或用于增强数据集以提高机器学习模型的鲁棒性。
  - 高斯噪声的基本原理
    高斯噪声的数学表示是基于高斯（正态）分布，该分布由两个参数定义：均值（mean）和标准差（sigma）。高斯分布的概率密度函数（PDF）为：
    $f(x|\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{(x-\mu)^2}{2\sigma^2} \right)$  
    其中，$\mu$是分布的均值，$\sigma$是分布的标准差。
### **(2)图片加噪过程**
- **生成高斯噪声矩阵**：根据图像的尺寸（行数、列数和通道数），生成一个与图像大小相同的高斯噪声矩阵。噪声矩阵的每个元素都是从均值为`mean`（0），标准差为`sigma`（0.5到1.5之间随机选取）的正态分布中生成的。
- **添加噪声到图像**：将生成的高斯噪声矩阵添加到原始图像上。这一步通过计算原始图像矩阵和噪声矩阵的逐元素和来实现。结果是一个包含高斯噪声的新图像。
### **(3)公式应用**
- 在实践中，为图像\(I\)添加高斯噪声可以表示为： $I_{noisy} = I + N(0, \sigma^2)$         
- 其中， $I_{noisy}$ 是加噪后的图像， $I$ 是原始图像， $N(0, \sigma^2)$ 是具有均值0和标准差 $\sigma$ 的高斯噪声矩阵。由于噪声矩阵的大小与原始图像相同，这一操作适用于每个像素和每个颜色通道。

- 该部分代码文件存于“代码\图片加噪”

## **$step3$ 统一图片大小**

统一图片大小为128 $*$ 128，这样方便了训练模型也将维度控制在一个合理的范围。

- 这部分代码保存在“代码\图片统一大小”

## **$step4$ 小波变换**

使用离散小波变换（Discrete Wavelet Transform，DWT）对图像进行处理，并保存处理后的小波系数。离散小波变换是一种用于信号和图像处理的重要工具，它可以将数据或图像分解为不同频率的成分，并同时保留时间或空间信息。这使得DWT非常适合于图像压缩和特征提取等应用。

### **(1)基本原理**

离散小波变换利用小波基函数通过尺度和位置参数的变化来分析信号。与傅里叶变换相比，小波变换能够提供信号的时频（或时空）表示，这意味着它能够揭示信号在不同尺度（频率）和不同时间点（或位置）的特性。

###  **(2)主要公式和推导**

DWT将信号分解为一系列小波子带，通常包括近似（低频）部分和细节（高频）部分。对于图像处理，这可以扩展到二维，分别得到水平（cH）、垂直（cV）和对角（cD）细节系数，以及近似（cA）系数。

 - **一维DWT**的基本步骤可以表示为：

 1. **尺度函数 $\phi(t)$ 和小波函数 $\psi(t)$**：

    - 尺度函数用于生成近似系数（低频信息）。
    - 小波函数用于生成细节系数（高频信息）。

 2. **离散小波变换公式**：

    近似系数 $a_{j,k}$ 和细节系数 $d_{j,k}$ 可以通过以下公式计算：

    - $a_{j,k} = \sum_{n} x[n] \cdot \phi_{j,k}[n]$  

    - $d_{j,k} = \sum_{n} x[n] \cdot \psi_{j,k}[n]$ 

      其中， $x[n]$ 是输入信号， $j$ 和 $k$ 分别代表尺度和位置参数， $\phi_{j,k}[n]$ 和 $\psi_{j,k}[n]$ 是尺度和小波函数在不同尺度和位置的表示。

 - **二维DWT**
   对图像应用时，会先对行应用一维DWT，然后对结果的每一列应用一维DWT。这一过程产生四个子带：LL（低频-低频，即近似系数cA）、LH（低频-高频，即水平细节系数cH）、HL（高频-低频，即垂直细节系数cV）和HH（高频-高频，即对角细节系数cD）。

 - **Haar小波**
   使用了`'haar'`小波，这是最简单的小波之一。Haar小波的尺度函数和小波函数分别为简单的阶跃函数和单个脉冲，这使得它在计算上非常高效，特别适合于教学和初步图像处理实验。

 - **Python中的DWT**
   在代码中，使用了`pywt.dwt2()`函数来对每个颜色通道的图像应用二维离散小波变换。这个函数返回四个系数：近似系数（cA），水平细节系数（cH），垂直细节系数（cV），和对角细节系数（cD），这些正好对应于二维DWT的四个子带。保存这些系数，能够捕获图像在不同分辨率下的重要信息，这对于许多图像处理任务（如压缩、降噪和特征提取）非常有用。

- 这部分代码保存在“代码\获取小波系数”

## **$step5$ 特征工程**

实现了一种特征工程方法，针对小波变换系数进行能量自适应阈值降维。在处理高维数据时，可以帮助提高算法效率和效果。下面是这个方法的原理、步骤和公式。

### **(1)原理**

这种方法的基本思想是通过降低数据的维度来减少计算量和改善算法的性能，同时尽量保留最重要的信息。在这种情况下，"重要性"是通过小波系数的能量来衡量的，能量高的系数被认为包含更多的信号信息。

### **(2)步骤**

1. **加载小波系数**：从`.npz`文件加载小波系数，包括近似系数(`cA`)、水平细节系数(`cH`)、垂直细节系数(`cV`)和对角细节系数(`cD`)。

2. **能量自适应阈值**：对于每组系数，计算所有系数值的平方（能量），然后根据能量大小和预设的比例（`ratio`）确定一个阈值，以便过滤掉能量较小的系数。这一步骤旨在保留最有信息量的系数，同时舍弃那些对于描述数据结构不重要的系数。

3. **降维**：将过滤后的系数降维到指定的目标维度（`target_dims`）。这一过程进一步减少了数据的维度，使得数据更适合于后续的处理和分析。

4. **保存处理后的数据**：将降维后的系数保存为Pandas DataFrame，然后导出到`.csv`文件，方便后续使用，如作为机器学习模型的输入。

### **(3)公式**

1. **能量计算**：对于给定的小波系数集 $C = \{c_1, c_2, ..., c_n\}$ ，计算每个系数的能量 $E_i = c_i^2$ 。

2. **阈值确定**：
   - 根据能量大小，计算能量的百分位数作为阈值： $T = \text{percentile}(E, (1 - \text{ratio}) \times 100)$ 。
   - 其中， $E$ 是所有系数能量的集合， $\text{ratio}$ 是预设的保留比例。

3. **系数过滤**：只保留那些其能量 $E_i$ 大于或等于阈值\(T\)的系数： $C_{filtered} = \{c_i | E_i \geq T\}$ 。

4. **降维与重组**：根据目标维度（`target_dims`）对过滤后的系数进行截断或填充，以确保最终的系数向量符合指定的维度要求。

通过这个过程，实现了基于小波系数能量的自适应阈值降维，既减少了数据的维度，又尽量保留了重要的信息，为后续的数据分析和机器学习任务提供了优化的特征集。

- 这部分代码保存在“代码\特征工程”

## **$step6$ 训练随机森林**

### (1)原理

随机森林由多个决策树组成，每棵树都是在数据集的一个随机子集上训练得到的。随机森林的预测结果是基于所有单个树的预测结果的平均值（对于回归问题）或是多数投票（对于分类问题）。这种方法可以有效减少过拟合，提高模型的泛化能力。

### (2)步骤

1. **载入数据**：从指定路径加载清洁和噪声数据，然后合并为一个大的数据集。

2. **划分数据集**：将数据集划分为训练集和测试集。

3. **设置网格搜索参数**：定义一系列想要优化的随机森林参数，如树的数量（`n_estimators`）、树的最大深度（`max_depth`）和分裂节点所需的最小样本数（`min_samples_split`）。

4. **初始化随机森林和网格搜索**：创建随机森林回归器的实例，并使用网格搜索（`GridSearchCV`）来找出最佳的模型参数。网格搜索通过交叉验证来评估不同参数组合的性能。

5. **模型训练**：使用训练数据和最佳参数训练随机森林模型。

6. **模型评估**：使用测试集评估模型性能，使用均方误差（MSE）作为评估指标。

### (3)公式

- **均方误差（MSE）**： $MSE = \frac{1}{n}\sum_{i=1}^{n}(Y_i - \hat{Y}_i)^2$ 
- 其中， $n$ 是样本数量， $Y_i$ 是第 $i$ 个样本的实际值， $\hat{Y}_i$ 是第 $i$ 个样本的预测值。

- **随机森林的预测**：对于回归问题，随机森林的预测值是所有决策树预测值的平均：$\hat{Y} = \frac{1}{T}\sum_{t=1}^{T}\hat{Y}_t$ 
- 其中， $T$ 是树的总数， $\hat{Y}_t$ 是第 $t$ 棵树的预测值。

通过这个过程，不仅实现了基于随机森林的回归分析，而且还通过网格搜索和交叉验证找到了最佳的模型参数，以期获得更好的预测性能。

- 这部分代码保存在“代码\machine_learning\Random_forest_Gauss.ipynb”
