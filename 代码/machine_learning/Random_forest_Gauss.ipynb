{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1d88c2beaed3773",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T11:47:02.953726800Z",
     "start_time": "2024-03-20T11:47:00.105768100Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.载入数据"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50f159d70b087e11"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_data(clean_csv_folder, noisy_csv_folder):\n",
    "    clean_data_frames = []\n",
    "    noisy_data_frames = []\n",
    "    \n",
    "    for filename in os.listdir(clean_csv_folder):\n",
    "        if filename.endswith('.csv'):\n",
    "            clean_csv_path = os.path.join(clean_csv_folder, filename)\n",
    "            noisy_csv_path = os.path.join(noisy_csv_folder, filename)\n",
    "            \n",
    "            clean_df = pd.read_csv(clean_csv_path)\n",
    "            noisy_df = pd.read_csv(noisy_csv_path)\n",
    "            \n",
    "            clean_data_frames.append(clean_df)\n",
    "            noisy_data_frames.append(noisy_df)\n",
    "    \n",
    "    # 合并所有DataFrame为单一DataFrame\n",
    "    clean_data = pd.concat(clean_data_frames, ignore_index=True)\n",
    "    noisy_data = pd.concat(noisy_data_frames, ignore_index=True)\n",
    "    \n",
    "    return clean_data.values, noisy_data.values\n",
    "\n",
    "clean_csv_folder = 'E:\\\\wavelet\\\\wavelet coefficient\\\\source_db6\\\\test_source_小波系数能量自适应阈值降维'  # Update this path\n",
    "noisy_csv_folder = 'E:\\\\wavelet\\\\wavelet coefficient\\\\Gauss_db6\\\\test_Gauss_小波系数能量自适应阈值降维'  # Update this path\n",
    "clean_data, noisy_data = load_data(clean_csv_folder, noisy_csv_folder)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T11:47:51.563998100Z",
     "start_time": "2024-03-20T11:47:48.603447700Z"
    }
   },
   "id": "b78988d35e0609ac",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.划分数据集"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b74b1eab01591485"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(noisy_data, clean_data, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T11:47:54.603659900Z",
     "start_time": "2024-03-20T11:47:54.596676500Z"
    }
   },
   "id": "bf7dc79eec3464f5",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.设置网格搜索参数"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9d805877dc387bd"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   5.9s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   5.5s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   5.7s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=  11.7s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=  11.1s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=  11.1s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=  17.7s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=  16.7s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=300; total time=  16.8s\n",
      "[CV] END max_depth=None, min_samples_split=4, n_estimators=100; total time=   5.0s\n",
      "[CV] END max_depth=None, min_samples_split=4, n_estimators=100; total time=   4.8s\n",
      "[CV] END max_depth=None, min_samples_split=4, n_estimators=100; total time=   4.8s\n",
      "[CV] END max_depth=None, min_samples_split=4, n_estimators=200; total time=  10.4s\n",
      "[CV] END max_depth=None, min_samples_split=4, n_estimators=200; total time=   9.8s\n",
      "[CV] END max_depth=None, min_samples_split=4, n_estimators=200; total time=   9.6s\n",
      "[CV] END max_depth=None, min_samples_split=4, n_estimators=300; total time=  15.4s\n",
      "[CV] END max_depth=None, min_samples_split=4, n_estimators=300; total time=  14.8s\n",
      "[CV] END max_depth=None, min_samples_split=4, n_estimators=300; total time=  14.6s\n",
      "[CV] END max_depth=None, min_samples_split=6, n_estimators=100; total time=   4.5s\n",
      "[CV] END max_depth=None, min_samples_split=6, n_estimators=100; total time=   4.3s\n",
      "[CV] END max_depth=None, min_samples_split=6, n_estimators=100; total time=   4.3s\n",
      "[CV] END max_depth=None, min_samples_split=6, n_estimators=200; total time=   9.2s\n",
      "[CV] END max_depth=None, min_samples_split=6, n_estimators=200; total time=   8.5s\n",
      "[CV] END max_depth=None, min_samples_split=6, n_estimators=200; total time=   8.7s\n",
      "[CV] END max_depth=None, min_samples_split=6, n_estimators=300; total time=  14.7s\n",
      "[CV] END max_depth=None, min_samples_split=6, n_estimators=300; total time=  12.9s\n",
      "[CV] END max_depth=None, min_samples_split=6, n_estimators=300; total time=  13.4s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   5.5s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   5.5s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=  11.2s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=  10.8s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=  11.0s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=  17.2s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=  17.4s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=300; total time=  17.2s\n",
      "[CV] END max_depth=10, min_samples_split=4, n_estimators=100; total time=   5.4s\n",
      "[CV] END max_depth=10, min_samples_split=4, n_estimators=100; total time=   4.9s\n",
      "[CV] END max_depth=10, min_samples_split=4, n_estimators=100; total time=   4.8s\n",
      "[CV] END max_depth=10, min_samples_split=4, n_estimators=200; total time=  11.1s\n",
      "[CV] END max_depth=10, min_samples_split=4, n_estimators=200; total time=  10.7s\n",
      "[CV] END max_depth=10, min_samples_split=4, n_estimators=200; total time=  10.4s\n",
      "[CV] END max_depth=10, min_samples_split=4, n_estimators=300; total time=  16.1s\n",
      "[CV] END max_depth=10, min_samples_split=4, n_estimators=300; total time=  14.5s\n",
      "[CV] END max_depth=10, min_samples_split=4, n_estimators=300; total time=  14.9s\n",
      "[CV] END max_depth=10, min_samples_split=6, n_estimators=100; total time=   4.5s\n",
      "[CV] END max_depth=10, min_samples_split=6, n_estimators=100; total time=   4.3s\n",
      "[CV] END max_depth=10, min_samples_split=6, n_estimators=100; total time=   4.3s\n",
      "[CV] END max_depth=10, min_samples_split=6, n_estimators=200; total time=   9.0s\n",
      "[CV] END max_depth=10, min_samples_split=6, n_estimators=200; total time=   8.5s\n",
      "[CV] END max_depth=10, min_samples_split=6, n_estimators=200; total time=   8.6s\n",
      "[CV] END max_depth=10, min_samples_split=6, n_estimators=300; total time=  13.7s\n",
      "[CV] END max_depth=10, min_samples_split=6, n_estimators=300; total time=  13.0s\n",
      "[CV] END max_depth=10, min_samples_split=6, n_estimators=300; total time=  13.0s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   5.8s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   5.7s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=  11.9s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=  11.1s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=  11.4s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=  17.6s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=  16.8s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=300; total time=  16.9s\n",
      "[CV] END max_depth=20, min_samples_split=4, n_estimators=100; total time=   5.1s\n",
      "[CV] END max_depth=20, min_samples_split=4, n_estimators=100; total time=   4.8s\n",
      "[CV] END max_depth=20, min_samples_split=4, n_estimators=100; total time=   4.9s\n",
      "[CV] END max_depth=20, min_samples_split=4, n_estimators=200; total time=  10.2s\n",
      "[CV] END max_depth=20, min_samples_split=4, n_estimators=200; total time=   9.7s\n",
      "[CV] END max_depth=20, min_samples_split=4, n_estimators=200; total time=   9.7s\n",
      "[CV] END max_depth=20, min_samples_split=4, n_estimators=300; total time=  15.3s\n",
      "[CV] END max_depth=20, min_samples_split=4, n_estimators=300; total time=  15.6s\n",
      "[CV] END max_depth=20, min_samples_split=4, n_estimators=300; total time=  15.0s\n",
      "[CV] END max_depth=20, min_samples_split=6, n_estimators=100; total time=   4.6s\n",
      "[CV] END max_depth=20, min_samples_split=6, n_estimators=100; total time=   4.5s\n",
      "[CV] END max_depth=20, min_samples_split=6, n_estimators=100; total time=   4.3s\n",
      "[CV] END max_depth=20, min_samples_split=6, n_estimators=200; total time=   9.6s\n",
      "[CV] END max_depth=20, min_samples_split=6, n_estimators=200; total time=   8.9s\n",
      "[CV] END max_depth=20, min_samples_split=6, n_estimators=200; total time=   8.8s\n",
      "[CV] END max_depth=20, min_samples_split=6, n_estimators=300; total time=  14.7s\n",
      "[CV] END max_depth=20, min_samples_split=6, n_estimators=300; total time=  13.2s\n",
      "[CV] END max_depth=20, min_samples_split=6, n_estimators=300; total time=  13.2s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=100; total time=   5.8s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=100; total time=   5.8s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=200; total time=  11.9s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=200; total time=  11.2s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=200; total time=  11.1s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=300; total time=  17.6s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=300; total time=  17.0s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=300; total time=  17.1s\n",
      "[CV] END max_depth=30, min_samples_split=4, n_estimators=100; total time=   5.1s\n",
      "[CV] END max_depth=30, min_samples_split=4, n_estimators=100; total time=   4.8s\n",
      "[CV] END max_depth=30, min_samples_split=4, n_estimators=100; total time=   4.8s\n",
      "[CV] END max_depth=30, min_samples_split=4, n_estimators=200; total time=  10.2s\n",
      "[CV] END max_depth=30, min_samples_split=4, n_estimators=200; total time=   9.8s\n",
      "[CV] END max_depth=30, min_samples_split=4, n_estimators=200; total time=  10.1s\n",
      "[CV] END max_depth=30, min_samples_split=4, n_estimators=300; total time=  16.1s\n",
      "[CV] END max_depth=30, min_samples_split=4, n_estimators=300; total time=  15.1s\n",
      "[CV] END max_depth=30, min_samples_split=4, n_estimators=300; total time=  14.9s\n",
      "[CV] END max_depth=30, min_samples_split=6, n_estimators=100; total time=   4.6s\n",
      "[CV] END max_depth=30, min_samples_split=6, n_estimators=100; total time=   4.2s\n",
      "[CV] END max_depth=30, min_samples_split=6, n_estimators=100; total time=   4.3s\n",
      "[CV] END max_depth=30, min_samples_split=6, n_estimators=200; total time=   9.1s\n",
      "[CV] END max_depth=30, min_samples_split=6, n_estimators=200; total time=   9.2s\n",
      "[CV] END max_depth=30, min_samples_split=6, n_estimators=200; total time=   9.3s\n",
      "[CV] END max_depth=30, min_samples_split=6, n_estimators=300; total time=  14.3s\n",
      "[CV] END max_depth=30, min_samples_split=6, n_estimators=300; total time=  13.1s\n",
      "[CV] END max_depth=30, min_samples_split=6, n_estimators=300; total time=  13.2s\n",
      "Best parameters found: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 4, 6]\n",
    "}\n",
    "\n",
    "# 初始化随机森林回归器\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# 初始化网格搜索\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=2)\n",
    "\n",
    "# 执行网格搜索\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 输出最佳参数\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "\n",
    "# 使用最佳参数的模型进行预测\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T12:06:57.768184700Z",
     "start_time": "2024-03-20T11:48:03.963417Z"
    }
   },
   "id": "c43cf723272b4436",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4.模型评估"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "762c3ea921e1e071"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 3044.4978512149482\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T12:07:07.769204800Z",
     "start_time": "2024-03-20T12:07:07.752190500Z"
    }
   },
   "id": "2a3feece30b2ec4e",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5.保存模型"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e1973a18db3c803"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已保存到 E:\\wavelet\\trained_models\\random_forest_regressor.joblib\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# 保存模型的路径\n",
    "model_save_path = 'E:\\\\wavelet\\\\trained_models\\\\random_forest_regressor.joblib'\n",
    "\n",
    "# 获取模型保存路径的目录部分\n",
    "model_save_dir = os.path.dirname(model_save_path)\n",
    "\n",
    "# 检查目录是否存在，如果不存在，则创建\n",
    "if not os.path.exists(model_save_dir):\n",
    "    os.makedirs(model_save_dir)\n",
    "    print(f\"创建目录：{model_save_dir}\")\n",
    "\n",
    "# 保存模型\n",
    "joblib.dump(grid_search.best_estimator_, model_save_path)\n",
    "\n",
    "print(f\"模型已保存到 {model_save_path}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T02:51:04.522551700Z",
     "start_time": "2024-03-18T02:51:04.198827900Z"
    }
   },
   "id": "f6323ca794c2b301",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.模型应用于验证集"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "856de9bdb5e20f14"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# 模型加载\n",
    "model_save_path = 'E:\\\\wavelet\\\\trained_models\\\\random_forest_regressor.joblib'\n",
    "loaded_model = joblib.load(model_save_path)\n",
    "\n",
    "# 验证集文件夹路径\n",
    "validation_data_folder = 'E:\\\\wavelet\\\\wavelet coefficient\\\\Gauss\\\\Validation_Gauss_小波系数能量自适应阈值降维'\n",
    "# \n",
    "# # 结果保存文件夹\n",
    "# result_save_folder = 'E:\\\\wavelet\\\\wavelet coefficient\\\\predictions\\\\validation'\n",
    "# if not os.path.exists(result_save_folder):\n",
    "#     os.makedirs(result_save_folder)\n",
    "# \n",
    "# def process_and_save_predictions(validation_data_folder, result_save_folder, model):\n",
    "#     for filename in os.listdir(validation_data_folder):\n",
    "#         if filename.endswith('.csv'):\n",
    "#             file_path = os.path.join(validation_data_folder, filename)\n",
    "#             df = pd.read_csv(file_path)\n",
    "#             \n",
    "#             # 假设模型预测基于特定的特征列\n",
    "#             # X = df[['feature1', 'feature2', ...]].values\n",
    "#             X = df.values  # 如果模型使用了所有列作为特征\n",
    "#             \n",
    "#             # 进行预测\n",
    "#             predictions = model.predict(X)\n",
    "#             \n",
    "#             # 将预测结果保存回新的DataFrame（如果需要保留其他列，请根据需要调整）\n",
    "#             result_df = pd.DataFrame(predictions, columns=['Predicted'])  # 调整列名和结构以匹配原文件\n",
    "#             # result_df = pd.concat([df, result_df], axis=1) # 如果需要原始数据和预测结果一起保存\n",
    "#             \n",
    "#             # 保存预测结果到新文件\n",
    "#             result_file_path = os.path.join(result_save_folder, f\"predicted_{filename}\")\n",
    "#             result_df.to_csv(result_file_path, index=False)\n",
    "#             print(f\"预测结果已保存到 {result_file_path}\")\n",
    "# \n",
    "# # 处理验证集数据并保存预测结果\n",
    "# process_and_save_predictions(validation_data_folder, result_save_folder, loaded_model)\n",
    "\n",
    "\n",
    "def process_and_aggregate_predictions(validation_data_folder, model):\n",
    "    aggregated_predictions = []  # 聚合预测结果的列表\n",
    "    for filename in os.listdir(validation_data_folder):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(validation_data_folder, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # 如果模型使用了所有列作为特征\n",
    "            X = df.values\n",
    "            \n",
    "            # 进行预测\n",
    "            predictions = model.predict(X)\n",
    "            \n",
    "            # 聚合预测结果\n",
    "            aggregated_predictions.extend(predictions)\n",
    "    \n",
    "    return np.array(aggregated_predictions)  # 返回聚合后的numpy数组\n",
    "\n",
    "# 使用聚合函数处理验证集数据\n",
    "y_val_pred = process_and_aggregate_predictions(validation_data_folder, loaded_model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T07:29:19.858155200Z",
     "start_time": "2024-03-18T07:29:14.883079Z"
    }
   },
   "id": "caa19a20219dc7af",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7.验证集测试评估"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd2faf6e26593404"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MSE: 1863.1722961889673\n",
      "Validation RMSE: 43.164479565830135\n",
      "Validation MAE: 23.88976176690688\n",
      "Validation R²: 0.43927595972815037\n"
     ]
    }
   ],
   "source": [
    "# 真实值文件夹路径\n",
    "true_data_folder = 'E:\\\\wavelet\\\\wavelet coefficient\\\\source\\\\Validation_source_小波系数能量自适应阈值降维'\n",
    "\n",
    "# 加载验证集真实值的函数\n",
    "def load_true_data(true_data_folder):\n",
    "    true_data_frames = []\n",
    "    for filename in os.listdir(true_data_folder):\n",
    "        if filename.endswith('.csv'):\n",
    "            true_csv_path = os.path.join(true_data_folder, filename)\n",
    "            true_df = pd.read_csv(true_csv_path)\n",
    "            true_data_frames.append(true_df)\n",
    "    \n",
    "    # 合并所有DataFrame为单一DataFrame\n",
    "    combined_true_data = pd.concat(true_data_frames, ignore_index=True)\n",
    "    \n",
    "    return combined_true_data.values  # 返回numpy数组\n",
    "\n",
    "# 加载验证集真实值\n",
    "y_val_true = load_true_data(true_data_folder)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# 计算评估指标\n",
    "mse = mean_squared_error(y_val_true, y_val_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_val_true, y_val_pred)\n",
    "r2 = r2_score(y_val_true, y_val_pred)\n",
    "\n",
    "# 打印评估指标\n",
    "print(f\"Validation MSE: {mse}\")\n",
    "print(f\"Validation RMSE: {rmse}\")\n",
    "print(f\"Validation MAE: {mae}\")\n",
    "print(f\"Validation R²: {r2}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T07:29:24.404662500Z",
     "start_time": "2024-03-18T07:29:23.498223Z"
    }
   },
   "id": "faea76bcd2e57343",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b33a7433278eedcd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
